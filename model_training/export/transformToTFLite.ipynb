{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "484aeac4-3eb6-4be1-bb85-143d4f23a5b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T21:31:52.652247Z",
     "iopub.status.busy": "2025-08-30T21:31:52.651111Z",
     "iopub.status.idle": "2025-08-30T21:32:00.658457Z",
     "shell.execute_reply": "2025-08-30T21:32:00.657425Z",
     "shell.execute_reply.started": "2025-08-30T21:31:52.652208Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.189 üöÄ Python-3.10.12 torch-2.0.1+cu118 CPU (Intel Xeon Processor (Cascadelake))\n",
      "YOLO11s summary (fused): 100 layers, 9,413,574 parameters, 0 gradients, 21.3 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/home/jupyter/datasphere/project/model_training/train_with_wandb/runs/detect/train5/weights/best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 6, 8400) (18.3 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 14...\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.65...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success ‚úÖ 5.6s, saved as '/home/jupyter/datasphere/project/model_training/train_with_wandb/runs/detect/train5/weights/best.onnx' (36.2 MB)\n",
      "\n",
      "Export complete (6.2s)\n",
      "Results saved to \u001b[1m/home/jupyter/work/resources/model_training/train_with_wandb/runs/detect/train5/weights\u001b[0m\n",
      "Predict:         yolo predict task=detect model=/home/jupyter/datasphere/project/model_training/train_with_wandb/runs/detect/train5/weights/best.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=/home/jupyter/datasphere/project/model_training/train_with_wandb/runs/detect/train5/weights/best.onnx imgsz=640 data=data.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/datasphere/project/model_training/train_with_wandb/runs/detect/train5/weights/best.onnx'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "path = \"/home/jupyter/datasphere/project/model_training/train_with_wandb/runs/detect/train5/weights/\"\n",
    "\n",
    "namePT = \"best.pt\"\n",
    "\n",
    "model = YOLO(path + namePT)  \n",
    "model.export(format=\"onnx\", opset=14) # –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deef9f5b-ed71-4ea1-a23b-5d915f60a2c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T21:32:00.661466Z",
     "iopub.status.busy": "2025-08-30T21:32:00.660061Z",
     "iopub.status.idle": "2025-08-30T21:32:20.884777Z",
     "shell.execute_reply": "2025-08-30T21:32:20.883833Z",
     "shell.execute_reply.started": "2025-08-30T21:32:00.661414Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Function `__call__` contains input name(s) x, y with unsupported characters which will be renamed to onnx_tf_prefix__model_23_add_1_x, onnx_tf_prefix__model_23_mul_2_y in the SavedModel.\n",
      "INFO:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: yolo_saved_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: yolo_saved_model/assets\n",
      "INFO:absl:Writing fingerprint to yolo_saved_model/fingerprint.pb\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from onnx_tf.backend import prepare\n",
    "\n",
    "nameONNX = 'best.onnx'\n",
    "\n",
    "onnx_path = path + nameONNX   # –ø—É—Ç—å –∫ ONNX\n",
    "onnx_model = onnx.load(onnx_path)\n",
    "tf_rep = prepare(onnx_model, strict=False)\n",
    "tf_rep.export_graph(\"yolo_saved_model\")      # —Å–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ saved_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f827e1a-556e-4e8e-a49a-bc94c04cfe1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T21:32:20.887983Z",
     "iopub.status.busy": "2025-08-30T21:32:20.886186Z",
     "iopub.status.idle": "2025-08-30T21:32:25.695583Z",
     "shell.execute_reply": "2025-08-30T21:32:25.694725Z",
     "shell.execute_reply.started": "2025-08-30T21:32:20.887945Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37859384"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "saved_model_dir = \"yolo_saved_model\"  \n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "\n",
    "# –†–∞–∑—Ä–µ—à–∞–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ ops –∏ Select TF Ops (Flex)\n",
    "converter.target_spec.supported_ops = [\n",
    "  tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "  tf.lite.OpsSet.SELECT_TF_OPS\n",
    "]\n",
    "converter.allow_custom_ops = True\n",
    "\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "open(\"model.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e525608-9d05-4711-a4b0-c38ed569ad1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d4bf45-6327-43d7-a800-975a58569f77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "28dd0eb4-ac25-45b3-a79e-2b798d9f0bc1",
   "metadata": {
    "tags": []
   },
   "source": [
    "tflite float32 to int8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc202bcf-59a3-4efd-91aa-6de0a29fc601",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-20T18:56:01.963451Z",
     "iopub.status.busy": "2025-09-20T18:56:01.961953Z",
     "iopub.status.idle": "2025-09-20T18:56:04.143364Z",
     "shell.execute_reply": "2025-09-20T18:56:04.141447Z",
     "shell.execute_reply.started": "2025-09-20T18:56:01.963395Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-20 18:56:03.043627: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2025-09-20 18:56:03.043693: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-09-20 18:56:03.043996: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: yolo_saved_model\n",
      "2025-09-20 18:56:03.060057: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2025-09-20 18:56:03.060108: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: yolo_saved_model\n",
      "2025-09-20 18:56:03.094768: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2025-09-20 18:56:03.223076: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: yolo_saved_model\n",
      "2025-09-20 18:56:03.315913: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 271917 microseconds.\n",
      "loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.2/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): error: 'tf.SplitV' op is neither a custom op nor a flex op\n",
      "loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.4/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): error: 'tf.SplitV' op is neither a custom op nor a flex op\n",
      "loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.6/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): error: 'tf.SplitV' op is neither a custom op nor a flex op\n",
      "loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.8/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): error: 'tf.SplitV' op is neither a custom op nor a flex op\n",
      "loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.10/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): error: 'tf.SplitV' op is neither a custom op nor a flex op\n",
      "loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.10/m/m.0/attn/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): error: 'tf.SplitV' op is neither a custom op nor a flex op\n",
      "loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.13/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): error: 'tf.SplitV' op is neither a custom op nor a flex op\n",
      "loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.16/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): error: 'tf.SplitV' op is neither a custom op nor a flex op\n",
      "loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.19/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): error: 'tf.SplitV' op is neither a custom op nor a flex op\n",
      "loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.22/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): error: 'tf.SplitV' op is neither a custom op nor a flex op\n",
      "loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.23/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): error: 'tf.SplitV' op is neither a custom op nor a flex op\n",
      "error: failed while converting: 'main': \n",
      "Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select \n",
      "TF Select ops: SplitV\n",
      "Details:\n",
      "\ttf.SplitV(tensor<1x128x80x80xf32>, tensor<2xi64>, tensor<i32>) -> (tensor<1x64x80x80xf32>, tensor<1x64x80x80xf32>) : {device = \"\"}\n",
      "\ttf.SplitV(tensor<1x256x40x40xf32>, tensor<2xi64>, tensor<i32>) -> (tensor<1x128x40x40xf32>, tensor<1x128x40x40xf32>) : {device = \"\"}\n",
      "\ttf.SplitV(tensor<1x4x128x400xf32>, tensor<3xi64>, tensor<i32>) -> (tensor<1x4x32x400xf32>, tensor<1x4x32x400xf32>, tensor<1x4x64x400xf32>) : {device = \"\"}\n",
      "\ttf.SplitV(tensor<1x512x20x20xf32>, tensor<2xi64>, tensor<i32>) -> (tensor<1x256x20x20xf32>, tensor<1x256x20x20xf32>) : {device = \"\"}\n",
      "\ttf.SplitV(tensor<1x64x160x160xf32>, tensor<2xi64>, tensor<i32>) -> (tensor<1x32x160x160xf32>, tensor<1x32x160x160xf32>) : {device = \"\"}\n",
      "\ttf.SplitV(tensor<1x66x8400xf32>, tensor<2xi64>, tensor<i32>) -> (tensor<1x64x8400xf32>, tensor<1x2x8400xf32>) : {device = \"\"}\n",
      "\n"
     ]
    },
    {
     "ename": "ConverterError",
     "evalue": "<unknown>:0: error: loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.2/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): 'tf.SplitV' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"PartitionedCall:\", \"PartitionedCall\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.2/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.4/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): 'tf.SplitV' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"PartitionedCall:\", \"PartitionedCall\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.4/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.6/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): 'tf.SplitV' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"PartitionedCall:\", \"PartitionedCall\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.6/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.8/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): 'tf.SplitV' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"PartitionedCall:\", \"PartitionedCall\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.8/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.10/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): 'tf.SplitV' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"PartitionedCall:\", \"PartitionedCall\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.10/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.10/m/m.0/attn/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): 'tf.SplitV' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"PartitionedCall:\", \"PartitionedCall\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.10/m/m.0/attn/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.13/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): 'tf.SplitV' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"PartitionedCall:\", \"PartitionedCall\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.13/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.16/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): 'tf.SplitV' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"PartitionedCall:\", \"PartitionedCall\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.16/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.19/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): 'tf.SplitV' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"PartitionedCall:\", \"PartitionedCall\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.19/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.22/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): 'tf.SplitV' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"PartitionedCall:\", \"PartitionedCall\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.22/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.23/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): 'tf.SplitV' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"PartitionedCall:\", \"PartitionedCall\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.23/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: failed while converting: 'main': \nSome ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select \nTF Select ops: SplitV\nDetails:\n\ttf.SplitV(tensor<1x128x80x80xf32>, tensor<2xi64>, tensor<i32>) -> (tensor<1x64x80x80xf32>, tensor<1x64x80x80xf32>) : {device = \"\"}\n\ttf.SplitV(tensor<1x256x40x40xf32>, tensor<2xi64>, tensor<i32>) -> (tensor<1x128x40x40xf32>, tensor<1x128x40x40xf32>) : {device = \"\"}\n\ttf.SplitV(tensor<1x4x128x400xf32>, tensor<3xi64>, tensor<i32>) -> (tensor<1x4x32x400xf32>, tensor<1x4x32x400xf32>, tensor<1x4x64x400xf32>) : {device = \"\"}\n\ttf.SplitV(tensor<1x512x20x20xf32>, tensor<2xi64>, tensor<i32>) -> (tensor<1x256x20x20xf32>, tensor<1x256x20x20xf32>) : {device = \"\"}\n\ttf.SplitV(tensor<1x64x160x160xf32>, tensor<2xi64>, tensor<i32>) -> (tensor<1x32x160x160xf32>, tensor<1x32x160x160xf32>) : {device = \"\"}\n\ttf.SplitV(tensor<1x66x8400xf32>, tensor<2xi64>, tensor<i32>) -> (tensor<1x64x8400xf32>, tensor<1x2x8400xf32>) : {device = \"\"}\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConverterError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3376/1255201448.py\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference_output_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mtflite_quant_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# –°–æ—Ö—Ä–∞–Ω—è–µ–º\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1063\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_and_export_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1066\u001b[0m     \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36m_convert_and_export_metrics\u001b[0;34m(self, convert_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_conversion_params_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m     \u001b[0melapsed_time_ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1388\u001b[0m       )\n\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1390\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_from_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36m_convert_from_saved_model\u001b[0;34m(self, graph_def)\u001b[0m\n\u001b[1;32m   1254\u001b[0m     \u001b[0mconverter_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquant_mode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverter_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1256\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconverter_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1257\u001b[0m     return self._optimize_tflite_model(\n\u001b[1;32m   1258\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_io\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_new_quantizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m           \u001b[0mreport_error_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mconverter_error\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# Re-throws the exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mreport_error_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mConverterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconverter_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconverter_error\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/convert.py\u001b[0m in \u001b[0;36mconvert_saved_model\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    933\u001b[0m   \u001b[0mmodel_flags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m   \u001b[0mconversion_flags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_conversion_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m   data = convert(\n\u001b[0m\u001b[1;32m    936\u001b[0m       \u001b[0mmodel_flags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m       \u001b[0mconversion_flags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/convert.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(model_flags, conversion_flags, input_data_str, debug_info_str, enable_mlir_converter)\u001b[0m\n\u001b[1;32m    365\u001b[0m               \u001b[0menable_mlir_converter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m           )\n\u001b[0;32m--> 367\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mconverter_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m   return _run_deprecated_conversion_binary(\n",
      "\u001b[0;31mConverterError\u001b[0m: <unknown>:0: error: loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.2/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): 'tf.SplitV' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"PartitionedCall:\", \"PartitionedCall\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.2/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.4/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): 'tf.SplitV' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"PartitionedCall:\", \"PartitionedCall\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.4/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.6/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): 'tf.SplitV' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"PartitionedCall:\", \"PartitionedCall\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.6/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.8/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): 'tf.SplitV' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"PartitionedCall:\", \"PartitionedCall\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.8/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.10/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): 'tf.SplitV' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"PartitionedCall:\", \"PartitionedCall\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.10/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.10/m/m.0/attn/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): 'tf.SplitV' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"PartitionedCall:\", \"PartitionedCall\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.10/m/m.0/attn/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.13/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): 'tf.SplitV' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"PartitionedCall:\", \"PartitionedCall\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.13/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.16/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): 'tf.SplitV' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"PartitionedCall:\", \"PartitionedCall\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.16/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.19/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): 'tf.SplitV' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"PartitionedCall:\", \"PartitionedCall\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.19/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.22/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): 'tf.SplitV' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"PartitionedCall:\", \"PartitionedCall\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.22/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.23/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): 'tf.SplitV' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"PartitionedCall:\", \"PartitionedCall\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"SplitV:\", \"onnx_tf_prefix_/model.23/Split@__inference___call___2271\"] at fused[\"PartitionedCall:\", \"PartitionedCall@__inference_signature_wrapper_2669\"]) at fused[\"PartitionedCall:\", \"PartitionedCall\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: failed while converting: 'main': \nSome ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select \nTF Select ops: SplitV\nDetails:\n\ttf.SplitV(tensor<1x128x80x80xf32>, tensor<2xi64>, tensor<i32>) -> (tensor<1x64x80x80xf32>, tensor<1x64x80x80xf32>) : {device = \"\"}\n\ttf.SplitV(tensor<1x256x40x40xf32>, tensor<2xi64>, tensor<i32>) -> (tensor<1x128x40x40xf32>, tensor<1x128x40x40xf32>) : {device = \"\"}\n\ttf.SplitV(tensor<1x4x128x400xf32>, tensor<3xi64>, tensor<i32>) -> (tensor<1x4x32x400xf32>, tensor<1x4x32x400xf32>, tensor<1x4x64x400xf32>) : {device = \"\"}\n\ttf.SplitV(tensor<1x512x20x20xf32>, tensor<2xi64>, tensor<i32>) -> (tensor<1x256x20x20xf32>, tensor<1x256x20x20xf32>) : {device = \"\"}\n\ttf.SplitV(tensor<1x64x160x160xf32>, tensor<2xi64>, tensor<i32>) -> (tensor<1x32x160x160xf32>, tensor<1x32x160x160xf32>) : {device = \"\"}\n\ttf.SplitV(tensor<1x66x8400xf32>, tensor<2xi64>, tensor<i32>) -> (tensor<1x64x8400xf32>, tensor<1x2x8400xf32>) : {device = \"\"}\n\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º float –º–æ–¥–µ–ª—å. –ú–æ–∂–Ω–æ –∏–∑ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ (SavedModel) –∏–ª–∏ .tflite\n",
    "# –ï—Å–ª–∏ —É —Ç–µ–±—è —É–∂–µ .tflite float32, –º–æ–∂–Ω–æ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —á–µ—Ä–µ–∑ from_concrete_functions –∏–ª–∏ from_saved_model\n",
    "\n",
    "pathToModel=\"yolo_saved_model\"\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º float –º–æ–¥–µ–ª—å. –ú–æ–∂–Ω–æ –∏–∑ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ (SavedModel) –∏–ª–∏ .tflite\n",
    "# –ï—Å–ª–∏ —É —Ç–µ–±—è —É–∂–µ .tflite float32, –º–æ–∂–Ω–æ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —á–µ—Ä–µ–∑ from_concrete_functions –∏–ª–∏ from_saved_model\n",
    "\n",
    "# pathToModel=\"model.tflite\"\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(pathToModel)\n",
    "# –ò–ª–∏, –µ—Å–ª–∏ –µ—Å—Ç—å keras –º–æ–¥–µ–ª—å:\n",
    "# converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "def representative_dataset():\n",
    "    for input_value in dataset.take(num_calibration_steps):\n",
    "        # input_value –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å numpy –º–∞—Å—Å–∏–≤ –Ω—É–∂–Ω–æ–π —Ñ–æ—Ä–º—ã, —Ç–∏–ø–∞ float32\n",
    "        yield [input_value]\n",
    "\n",
    "converter.representative_dataset = representative_dataset\n",
    "\n",
    "# –ß—Ç–æ–±—ã –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å —á—Ç–æ –≤—Å—ë quantized –≤ int8:\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8  # –∏–ª–∏ tf.uint8, –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –º–æ–¥–µ–ª–∏\n",
    "converter.inference_output_type = tf.int8\n",
    "\n",
    "tflite_quant_model = converter.convert()\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º\n",
    "with open('model_int8.tflite', 'wb') as f:\n",
    "    f.write(tflite_quant_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be2670e-a68f-4201-890b-91a3d2b3d33b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
